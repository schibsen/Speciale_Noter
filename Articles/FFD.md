

#Pub2020
#JointCNN #DetectionModel #OpenSourceCode #OpenSourceData #BinaryClassification
# Title 
On the Detection of Digital Face Manipulation

code and data
https://cvlab.cse.msu.edu/project-ffd.html


# Summary
Proposal
- Instead of simply using multi-task learning to simultaneously detect manipulated images and predict the manipulated mask (regions), we propose to utilize an attention mechanism to process and improve the feature maps for the classification task
- the learned attention maps highlight the informative regions to further improve the binary classification(genuine face v. fake face), and also visualize the manipulated regions

Three main types of facial forgery attacks 
1. Physical spoofing
	- face printed on a paper
	- replaying image/video on a phone 
	- 3D mask 
	- [8,24,34,35]
2. Adversarial face attacks
- evade automated face matchers
- [18,20,37,57]
3. Digital Manipulation attacks 
- Variational Autoencoders (VAEs)
- Generative Adversarial Networks (GANs)
- entirely or partially modified photorealistic face images. 

Four categories of Digital face manipulation 
1. expression swap
- 3D face reconstruction and animation methods 
- [17,32,48,64]
- Face2Face [47]
2. identity swap
- FaceSwap [47.53]
- DeepFakes
3. attribute manipulation
- edits gender, age skin color, hair and glasses
- image translation is done via the adversarial framework of GANs [23,62,63]
- same for manipulation in a given context [10,45]
- FaceApp [4], 28 filters to modify specific attributes
4. entire face synthesis 
- GANs
- [15,25,26]

Dataset
- Diverse Fake Face Dataset [[Datasets/DFFD]]
- 2.6 million images from all four categories 
1. Real face images 
	- [[FFHQ]]
	- [[CelebA]]
	- [[FaceForensics++]] (1000 videos)
2. Identity and expression swap 
	- [[FaceForensics++]] (3000 manipulated videos)
		- identy swap: [[FaceSwap]] and [[Deepfake]]
		- expression swap: [[Face2Face]]
		- identity swap: [[DFL]] (Deep Face Lab) from public website [1]
3. Attributes mnaipulation
	- [[FFHQ]] (4000 faces)
	- [[CelebA]] (2000 face images)
	- [[FaceAPP]] (28 filters facial attribute manipulation, retouching ??)
	- [[StarGAN]] (1 CelebA image to 40 generated face images )--> 80000
		- a GAN based image-to-image translation 
4. Entire Face synthesis
	- PGGAN
	- StyleGAN

Preprocessing of Data 
- estimate bounding box and 5 landmarks for each image 
- InsightFace [21]

Implementation details 
-  backbone networks: XceptionNet [16]
- VGG16[44]
- pre-trained on ImageNet
- fine-tuned on DFFD

Metrics 
- EER
- AUC of ROC
- TDR at FDR of 0.01%
- TDR at FDR of 0.1%
- PBCA 
- IoU
- Cosine similarity btw. two vectorized maps
- IINC (novel metric)

IINC 
>$IINC= \frac{1}{3-|\mathbf{U}|}\cdot 
\begin{cases}
0 &,\text{if } \overline{\mathbf{M}_\text{gt}} = 0 \text{ and } \overline{\mathbf{M}_\text{att}} = 0\\
1 &,\text{if } \overline{\mathbf{M}_\text{gt}} = 0 \text{ xor }\, \overline{\mathbf{M}_\text{att}} = 0\\
\left(2- \frac{|\mathbf{I}|}{| \mathbf{M}_\text{att}|} - \frac{|\mathbf{I}|}{|\mathbf{M}_\text{gt}|}\right)&, \text{ otherwise}
\end{cases}$
>where $\mathbf{I}$ and $\mathbf{U}$ are the intersection and union between the ground truth map, $\mathbf{M}_\text{gt}$, and the predicted map, $\mathbf{M}_\text{att}$, respectively. 
>$\overline{\mathbf{M}}$ and $|\mathbf{M}|$ are the mean and the $L_1$ norm of $\mathbf{M}$, respectively. 
>The two fractional terms measure the ratio of the area of the intersection w.r.t. the area of each map, respectively. 


Assumption
- a well-learned network would gather different information spatially in order to detect manipulated faces 
Hypothesis 
- thus correctly estimating this spatial information can enable the network to focus on these important spatial regions to make its decision. 
- thereby locating the manipulated regions by estimating an image-specific attention map

Quantification / Metrics 
- a novel metric for attention map accuracy evaluation

Anticipation
- that the predicted attention maps for manipulated face images and videos would reveal hints about the type, magnitude and even intention of the manipulation. 

Contributions
- A comprehensive fake face dataset including 0.8M real and 1.6M fake faces generated by a diverse set of face modification methods and an accompanying evaluation protocol
- a novel attention-based layer to improve classification performance and produce an attention map indicating the manipulated facial regions 
- A novel metric, termed Inverse Intersection Non-Containment (IINC) for evaluating attention maps that produces a more coherent evaluation than existing metrics.
- SOTA performance of digital facial forgery detection for both seen and unseen manipulation methods 

Related work
- graphic based approaches 
	- identity or expression transfer 
	- Thies et al. [46] - expression swap
	- Face2Face[47] - expression 
	- Extended work[27] - head position, rotation, expression and eye blinking transfer
	- "Synthesizing Obama" [45] animates face(output) based on audio signal(input) (Not sure if it is placed right)
	- FaceSwap[?] identity swap, expression preservation
- Deep learning techniques 
	- ZAO [5]
	- FaceAPP [4]
	- GAN-based methods [25,26,49]

Fake Face Benchmarks, i.e. Datasets ? ??
- Zhou et al. [61], face swapped images
- FaceForensics(++)[41,42], video-based face manipulation
- Own Dataset
	- identity+expression swap , FaceForensics++[42]
	- face attribute manipulated images, FaceAPP[4]
	- complete fake face images, StyleGAN[26] snd PGGAN[25]

Proposal
- pose manipulated face detection as a **binary classification** problem using a CNN-based network 
- we propose to utilize the attention mechanism to process the feature maps of the classifier model. 
	- the learned attention maps can highlight the regions in an image which influence the CNN's decision, and further be used to guide the CNN to discover more discriminative features.

Motivation for the attention map 

# References


# PDF article
![[FFD On the Detection of Digital Face Manipulation.pdf]]
